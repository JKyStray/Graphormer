#!/usr/bin/env python3
"""
Adaptor-Only Training Script
Trains only the tFold adaptor layers while keeping the rest of the model frozen.
"""

import os
import torch
import torch.nn as nn
import torch.optim as optim
from pathlib import Path
import time
import argparse
from datetime import datetime
import torch.multiprocessing as mp
import matplotlib.pyplot as plt
import numpy as np

# NEW: Import the logger
from common.logger import Logger

def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='Adaptor-Only Training')
    
    # è®­ç»ƒå‚æ•° - é€‚é…å™¨è®­ç»ƒä¸“ç”¨è°ƒæ•´
    parser.add_argument('--epochs', type=int, default=5,
                       help='Number of training epochs (increased for adaptor training)')
    parser.add_argument('--lr', type=float, default=5e-4,
                       help='Learning rate (higher for adaptor-only training)')
    parser.add_argument('--weight_decay', type=float, default=1e-5,
                       help='Weight decay (lighter for small parameter set)')
    parser.add_argument('--log_freq', type=int, default=10,
                       help='Log every N batches')
    parser.add_argument('--batches_per_epoch', type=int, default=300,
                       help='Number of batches per epoch (reduced for adaptor training)')
    
    # å†…å­˜ä¼˜åŒ–å‚æ•° - é€‚é…å™¨è®­ç»ƒå¯ä»¥æ›´æ¿€è¿›
    parser.add_argument('--max_tokens_per_gpu', type=int, default=2048,
                       help='More aggressive memory setting for adaptor training')
    parser.add_argument('--gradient_accumulation', type=int, default=2,
                       help='Gradient accumulation steps (reduced for adaptor training)')
    
    # è·¯å¾„å‚æ•°
    parser.add_argument('--checkpoint_path', type=str, 
                       default='checkpoints/checkpoint-520k.pth',
                       help='Path to base checkpoint (adaptor layers expected to be missing)')
    parser.add_argument('--save_path', type=str,
                       default='checkpoints/checkpoint-520k-adaptor-trained.pth',
                       help='Path to save adaptor-trained checkpoint')
    
    # NEW: Add log_path argument
    parser.add_argument('--log_path', type=str,
                       default='logs/adaptor_training.log',
                       help='Path to save log file')
    
    # NEW: Add console log flag
    parser.add_argument('--console-log', action='store_true', default=True,
                       help='Enable console logging')
    parser.add_argument('--no-console-log', action='store_false', dest='console_log',
                       help='Disable console logging')
    
    return parser.parse_args()

def plot_training_loss(loss_history, batch_history, save_path):
    """
    Create and save a plot showing training loss vs batch number
    
    Args:
        loss_history: List of individual batch loss values
        batch_history: List of corresponding global batch numbers
        save_path: Path to save the plot
    """
    try:
        plt.figure(figsize=(15, 10))
        
        # Create the main loss plot
        plt.subplot(2, 1, 1)
        
        # Plot individual batch losses with transparency
        plt.plot(batch_history, loss_history, 'b-', linewidth=0.5, alpha=0.3, label='Individual Batch Loss')
        
        # Add rolling averages with different window sizes
        if len(loss_history) > 20:
            # Short-term rolling average (last 20 batches)
            window_short = min(20, len(loss_history) // 5)
            rolling_short = []
            for i in range(len(loss_history)):
                start_idx = max(0, i - window_short + 1)
                rolling_short.append(np.mean(loss_history[start_idx:i+1]))
            plt.plot(batch_history, rolling_short, 'r-', linewidth=2, label=f'Rolling Avg ({window_short} batches)')
            
        if len(loss_history) > 50:
            # Long-term rolling average (last 50 batches)
            window_long = min(50, len(loss_history) // 3)
            rolling_long = []
            for i in range(len(loss_history)):
                start_idx = max(0, i - window_long + 1)
                rolling_long.append(np.mean(loss_history[start_idx:i+1]))
            plt.plot(batch_history, rolling_long, 'g-', linewidth=2, label=f'Rolling Avg ({window_long} batches)')
        
        plt.xlabel('Global Batch Number')
        plt.ylabel('Loss')
        plt.title('Adaptor Training Loss vs Global Batch Number (Cross-Epoch Rolling Averages)')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # Create a subplot showing recent loss (last 20% of training)
        plt.subplot(2, 1, 2)
        recent_start = max(0, len(loss_history) - len(loss_history) // 5)
        recent_loss = loss_history[recent_start:]
        recent_batches = batch_history[recent_start:]
        
        plt.plot(recent_batches, recent_loss, 'b-', linewidth=0.8, alpha=0.6, label='Recent Batch Loss')
        
        # Add short rolling average for recent data
        if len(recent_loss) > 10:
            window_recent = min(10, len(recent_loss) // 2)
            rolling_recent = []
            for i in range(len(recent_loss)):
                start_idx = max(0, i - window_recent + 1)
                rolling_recent.append(np.mean(recent_loss[start_idx:i+1]))
            plt.plot(recent_batches, rolling_recent, 'r-', linewidth=2, label=f'Rolling Avg ({window_recent} batches)')
            
        plt.xlabel('Global Batch Number')
        plt.ylabel('Loss')
        plt.title('Recent Adaptor Training Loss (Last 20% of Training)')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # Save the plot
        plot_path = str(save_path).replace('.pth', '_adaptor_loss_plot.png')
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        return plot_path
        
    except Exception as e:
        print(f"Error creating loss plot: {e}")
        return None

def save_loss_data(loss_history, batch_history, save_path):
    """
    Save raw loss data to a file for further analysis
    
    Args:
        loss_history: List of individual batch loss values
        batch_history: List of corresponding global batch numbers
        save_path: Path to save the data file
    """
    try:
        import json
        
        data = {
            'batch_numbers': batch_history,
            'loss_values': loss_history,
            'metadata': {
                'training_type': 'adaptor_only',
                'total_batches': len(batch_history),
                'min_loss': min(loss_history) if loss_history else None,
                'max_loss': max(loss_history) if loss_history else None,
                'final_loss': loss_history[-1] if loss_history else None,
                'timestamp': datetime.now().isoformat()
            }
        }
        
        data_path = str(save_path).replace('.pth', '_adaptor_loss_data.json')
        with open(data_path, 'w') as f:
            json.dump(data, f, indent=2)
        
        return data_path
        
    except Exception as e:
        print(f"Error saving loss data: {e}")
        return None

def setup_memory_config(max_tokens_per_gpu, logger):
    """è®¾ç½®å†…å­˜é…ç½®"""
    from common import config as cfg
    
    original_max_tokens = cfg.max_tokens_per_gpu
    
    # è®¾ç½®å†…å­˜é…ç½®  
    cfg.max_tokens_per_gpu = max_tokens_per_gpu
    cfg.max_tokens_per_sample = 256
    
    effective_batch_size = cfg.max_tokens_per_gpu // cfg.max_tokens_per_sample
    
    logger.info(f"ğŸ”§ å†…å­˜é…ç½®:")
    logger.info(f"   max_tokens_per_gpu: {cfg.max_tokens_per_gpu} (åŸå§‹: {original_max_tokens})")
    logger.info(f"   max_tokens_per_sample: {cfg.max_tokens_per_sample}")
    logger.info(f"   æœ‰æ•ˆæ‰¹æ¬¡å¤§å°: {effective_batch_size}")
    
    return effective_batch_size

def setup_adaptor_training(model, logger):
    """è®¾ç½®é€‚é…å™¨è®­ç»ƒ - åªè®­ç»ƒtFoldé€‚é…å™¨å±‚"""
    total_params = 0
    trainable_params = 0
    frozen_params = 0
    adaptor_layer_names = ['tfold_expand', 'tfold_expand_pair']
    
    # é¦–å…ˆå†»ç»“æ‰€æœ‰å‚æ•°
    for param in model.parameters():
        param.requires_grad = False
        total_params += param.numel()
        frozen_params += param.numel()
    
    # ç„¶ååªè§£å†»é€‚é…å™¨å±‚
    for name, param in model.named_parameters():
        # æ£€æŸ¥æ˜¯å¦æ˜¯é€‚é…å™¨å±‚
        if any(adaptor_name in name for adaptor_name in adaptor_layer_names):
            param.requires_grad = True
            trainable_params += param.numel()
            frozen_params -= param.numel()
            logger.info(f"âœ… Trainable: {name} - {param.shape}")
        else:
            # æ˜¾ç¤ºä¸€äº›ä¸»è¦çš„å†»ç»“å±‚ï¼ˆä½†ä¸æ˜¯å…¨éƒ¨ï¼‰
            if any(keyword in name for keyword in ['st_module.layers.0', 'st_module.layers.1', 'x1d_proj', 'x2d_proj']):
                logger.info(f"â„ï¸ Frozen: {name[:50]}... - {param.shape}")
    
    training_ratio = (trainable_params / total_params) * 100
    
    logger.info(f"\nğŸ“Š é€‚é…å™¨è®­ç»ƒå‚æ•°ç»Ÿè®¡:")
    logger.info(f"   æ€»å‚æ•°: {total_params:,}")
    logger.info(f"   å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    logger.info(f"   å†»ç»“å‚æ•°: {frozen_params:,}")
    logger.info(f"   è®­ç»ƒæ¯”ä¾‹: {training_ratio:.2f}%")
    
    if trainable_params == 0:
        logger.error("âŒ æ²¡æœ‰æ‰¾åˆ°é€‚é…å™¨å±‚ï¼æ£€æŸ¥æ¨¡å‹ç»“æ„ã€‚")
        raise ValueError("No adaptor layers found for training")
    
    return trainable_params

def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    args = parse_args()
    
    # NEW: Setup logger
    log_file_path = Path(args.log_path)
    log_file_path.parent.mkdir(parents=True, exist_ok=True)
    logger = Logger(log_file_path, console=args.console_log)

    logger.info("ğŸ¯ é€‚é…å™¨è®­ç»ƒ")
    logger.info("=" * 80)
    logger.info("ğŸ”¥ ç‰¹ç‚¹:")
    logger.info("   - åªè®­ç»ƒtFoldé€‚é…å™¨å±‚ (tfold_expand, tfold_expand_pair)")
    logger.info("   - å†»ç»“æ‰€æœ‰å…¶ä»–æ¨¡å‹å‚æ•°")
    logger.info(f"   - å†…å­˜è®¾ç½®: {args.max_tokens_per_gpu} tokens")
    logger.info(f"   - å­¦ä¹ ç‡: {args.lr} (é€‚é…å™¨è®­ç»ƒä¼˜åŒ–)")
    logger.info(f"   - æ¢¯åº¦ç´¯ç§¯: {args.gradient_accumulation}æ­¥")
    logger.info("   - æœŸæœ›é€‚é…å™¨å±‚åœ¨åŸºç¡€æ£€æŸ¥ç‚¹ä¸­ç¼ºå¤±")
    logger.info(f"   - è®­ç»ƒè½®æ•°: {args.epochs} epochs")
    logger.info("")
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logger.info(f"è®¾å¤‡: {device}")
    
    if torch.cuda.is_available():
        total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
        logger.info(f"GPUå†…å­˜: {total_memory:.1f} GB")
        torch.cuda.empty_cache()
        logger.info(f"åˆå§‹GPUä½¿ç”¨: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB")
    logger.info("")
    
    try:
        # 1. è®¾ç½®å†…å­˜é…ç½®
        logger.info("ğŸ”§ é…ç½®å†…å­˜è®¾ç½®...")
        effective_batch_size = setup_memory_config(args.max_tokens_per_gpu, logger)
        
        # 2. åŠ è½½æ•°æ® - ä½¿ç”¨FiniteTrainLoaderé¿å…æ— é™è®­ç»ƒ
        logger.info("\nğŸ“¦ åŠ è½½æ•°æ®...")
        from data_provider.train_loader import FiniteTrainLoader
        
        train_loader = FiniteTrainLoader(batches_per_epoch=args.batches_per_epoch)
        logger.info(f"âœ… æœ‰é™æ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸ")
        logger.info(f"   æ‰¹æ¬¡å¤§å°: {effective_batch_size}")
        logger.info(f"   æ¯è½®æ‰¹æ¬¡æ•°: {args.batches_per_epoch}")
        logger.info(f"   ç­‰æ•ˆæ‰¹æ¬¡å¤§å° (ç´¯ç§¯å): {effective_batch_size * args.gradient_accumulation}")
        
        # 3. åˆ›å»ºæ¨¡å‹
        logger.info("\nğŸ—ï¸ åˆ›å»ºæ¨¡å‹...")
        from model.main_model import MainModel
        
        model = MainModel(d_model=768, d_pair=256, n_layer=12, n_heads=32)
        
        # 4. åŠ è½½åŸºç¡€æ£€æŸ¥ç‚¹ (é€‚é…å™¨å±‚é¢„æœŸç¼ºå¤±)
        logger.info(f"\nğŸ“‚ åŠ è½½åŸºç¡€æ£€æŸ¥ç‚¹...")
        if not Path(args.checkpoint_path).exists():
            logger.error(f"âŒ åŸºç¡€æ£€æŸ¥ç‚¹ä¸å­˜åœ¨: {args.checkpoint_path}")
            raise FileNotFoundError(f"Checkpoint not found: {args.checkpoint_path}")
        
        checkpoint = torch.load(args.checkpoint_path, map_location='cpu')
        state_dict = checkpoint.get('state_dict', checkpoint)
        
        # NEW: Fix for DataParallel prefix issue
        # If keys start with 'module.', it's from a DataParallel-saved model
        if list(state_dict.keys())[0].startswith('module.'):
            logger.info("ğŸ”§ Detected 'module.' prefix in checkpoint keys. Removing it.")
            from collections import OrderedDict
            new_state_dict = OrderedDict()
            for k, v in state_dict.items():
                name = k[7:] # remove `module.`
                new_state_dict[name] = v
            state_dict = new_state_dict

        # Load state dict with strict=False - adaptor layers are expected to be missing
        logger.info("ğŸ”§ Loading checkpoint with strict=False (adaptor layers expected to be missing).")
        missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)
        
        logger.info(f"  âœ… Checkpoint loaded.")
        
        # Check for adaptor layers in missing keys (this is expected and good)
        adaptor_missing_keys = [key for key in missing_keys if any(adaptor in key for adaptor in ['tfold_expand', 'tfold_expand_pair'])]
        non_adaptor_missing_keys = [key for key in missing_keys if key not in adaptor_missing_keys]
        
        if adaptor_missing_keys:
            logger.info("     âœ… Expected missing adaptor layers (will be randomly initialized):")
            for key in adaptor_missing_keys:
                logger.info(f"       - {key}")
        
        if non_adaptor_missing_keys:
            logger.info("     âš ï¸ Unexpected missing non-adaptor keys:")
            for key in non_adaptor_missing_keys[:5]:  # Show first 5
                logger.info(f"       - {key}")
            if len(non_adaptor_missing_keys) > 5:
                logger.info("       - ... and others")
        
        if unexpected_keys:
            logger.info("     âš ï¸ Unexpected keys (in checkpoint but not model):")
            for key in unexpected_keys:
                logger.info(f"       - {key}")

        model = model.to(device)
        if torch.cuda.is_available():
            logger.info(f"æ¨¡å‹åŠ è½½åGPUå†…å­˜: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB")
        
        # 5. è®¾ç½®é€‚é…å™¨è®­ç»ƒ
        trainable_params = setup_adaptor_training(model, logger)
        
        # 6. ä¼˜åŒ–å™¨ - ä¸“ä¸ºé€‚é…å™¨è®­ç»ƒä¼˜åŒ–
        trainable_param_list = [p for p in model.parameters() if p.requires_grad]
        
        # é€‚é…å™¨è®­ç»ƒä½¿ç”¨æ›´é«˜çš„å­¦ä¹ ç‡
        optimizer = optim.Adam(
            trainable_param_list,
            lr=args.lr,
            weight_decay=args.weight_decay,
            eps=1e-8
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler = optim.lr_scheduler.CosineAnnealingLR(
            optimizer, 
            T_max=args.epochs,
            eta_min=1e-5
        )
        
        # 7. è®­ç»ƒå¾ªç¯
        logger.info(f"\nğŸ¯ å¼€å§‹é€‚é…å™¨è®­ç»ƒ...")
        logger.info(f"é…ç½®: lr={args.lr}, æ¢¯åº¦ç´¯ç§¯={args.gradient_accumulation}æ­¥")
        logger.info(f"æ¯è½®æ‰¹æ¬¡æ•°: {args.batches_per_epoch}")
        logger.info(f"ç­‰æ•ˆæ‰¹æ¬¡å¤§å°: {effective_batch_size * args.gradient_accumulation}")
        logger.info("=" * 60)
        
        model.train()
        
        # NEW: Initialize loss tracking
        loss_history = []
        batch_history = []
        global_batch_count = 0  # Track global batch number across all epochs
        
        for epoch in range(args.epochs):
            epoch_start_time = time.time()
            epoch_loss = 0.0
            batch_count = 0
            accumulated_loss = 0.0
            
            logger.info(f"\nğŸ“Š Epoch {epoch + 1}/{args.epochs}")
            current_lr = optimizer.param_groups[0]['lr']
            logger.info(f"å½“å‰å­¦ä¹ ç‡: {current_lr:.6e}")
            
            # ä½¿ç”¨FiniteTrainLoaderï¼Œä¼šè‡ªåŠ¨åœ¨args.batches_per_epochååœæ­¢
            for batch_idx, batch in enumerate(train_loader):
                try:
                    # æ¢¯åº¦ç´¯ç§¯å¾ªç¯
                    if batch_count % args.gradient_accumulation == 0:
                        optimizer.zero_grad()
                    
                    # ç§»åˆ°è®¾å¤‡
                    for key in batch:
                        if isinstance(batch[key], torch.Tensor):
                            batch[key] = batch[key].to(device, non_blocking=True)
                    
                    # å‰å‘ä¼ æ’­
                    output = model(batch, compute_loss=True)
                    loss = output['loss'] / args.gradient_accumulation
                    
                    if torch.isnan(loss) or torch.isinf(loss):
                        logger.info(f"âš ï¸ è·³è¿‡æ— æ•ˆloss: {loss.item()}")
                        continue
                    
                    # åå‘ä¼ æ’­
                    loss.backward()
                    accumulated_loss += loss.item()
                    
                    # æ¯accumulationæ­¥æ›´æ–°ä¸€æ¬¡æƒé‡
                    if (batch_count + 1) % args.gradient_accumulation == 0:
                        # æ¢¯åº¦è£å‰ª
                        torch.nn.utils.clip_grad_norm_(trainable_param_list, max_norm=1.0)
                        optimizer.step()
                        
                        epoch_loss += accumulated_loss
                        update_count = (batch_count + 1) // args.gradient_accumulation
                        
                        # NEW: Record individual batch loss for plotting (every gradient update)
                        loss_history.append(accumulated_loss)
                        batch_history.append(global_batch_count)
                        global_batch_count += 1
                        
                        # æ—¥å¿—
                        if update_count % args.log_freq == 0:
                            avg_loss = epoch_loss / update_count
                            gpu_mem = torch.cuda.memory_allocated(0) / 1e9 if torch.cuda.is_available() else 0
                            
                            log_msg = (f"  Update {update_count:3d} | "
                                       f"Batch Loss: {accumulated_loss:.6f} | "
                                       f"Epoch Avg: {avg_loss:.6f} | "
                                       f"LR: {current_lr:.6e} | "
                                       f"GPU: {gpu_mem:.1f}GB")
                            logger.info(log_msg)
                        
                        accumulated_loss = 0.0
                    
                    batch_count += 1
                
                except RuntimeError as e:
                    if "out of memory" in str(e):
                        logger.info(f"âš ï¸ GPUå†…å­˜ä¸è¶³ï¼Œæ¸…ç†ç¼“å­˜...")
                        torch.cuda.empty_cache()
                        continue
                    else:
                        raise e
                except Exception as e:
                    logger.info(f"âŒ æ‰¹æ¬¡å¤±è´¥: {e}")
                    torch.cuda.empty_cache()
                    continue
            
            # Epochç»Ÿè®¡
            epoch_time = time.time() - epoch_start_time
            update_count = batch_count // args.gradient_accumulation
            avg_loss = epoch_loss / max(update_count, 1)
            
            logger.info(f"\nğŸ“ˆ Epoch {epoch + 1} å®Œæˆ:")
            logger.info(f"   å¹³å‡æŸå¤±: {avg_loss:.6f}")
            logger.info(f"   å¤„ç†æ‰¹æ¬¡: {batch_count}")
            logger.info(f"   æƒé‡æ›´æ–°: {update_count}")
            logger.info(f"   ç”¨æ—¶: {epoch_time:.1f}s")
            
            # æ›´æ–°å­¦ä¹ ç‡
            scheduler.step()
            
            # ä¿å­˜ä¸­é—´æ£€æŸ¥ç‚¹
            if epoch < args.epochs - 1:
                intermediate_path = args.save_path.replace('.pth', f'_epoch_{epoch+1}.pth')
                checkpoint = {
                    'state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'scheduler_state_dict': scheduler.state_dict(),
                    'epoch': epoch + 1,
                    'loss': avg_loss,
                    'metadata': {
                        'training_type': 'adaptor_only',
                        'timestamp': datetime.now().isoformat(),
                        'batches_per_epoch': args.batches_per_epoch,
                        'lr': args.lr,
                        'trainable_params': trainable_params
                    }
                }
                torch.save(checkpoint, intermediate_path)
                logger.info(f"ğŸ’¾ ä¸­é—´æ£€æŸ¥ç‚¹ä¿å­˜: {intermediate_path}")
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        Path(args.save_path).parent.mkdir(exist_ok=True)
        final_checkpoint = {
            'state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'scheduler_state_dict': scheduler.state_dict(),
            'epoch': args.epochs,
            'loss': avg_loss,
            'metadata': {
                'training_type': 'adaptor_only_final',
                'timestamp': datetime.now().isoformat(),
                'total_trainable_params': trainable_params,
                'batches_per_epoch': args.batches_per_epoch,
                'lr': args.lr,
                'note': 'Only tFold adaptor layers trained (tfold_expand, tfold_expand_pair)'
            }
        }
        torch.save(final_checkpoint, args.save_path)
        
        # NEW: Generate and save loss plot
        if loss_history:
            logger.info(f"\nğŸ“Š ç”ŸæˆæŸå¤±å›¾...")
            plot_path = plot_training_loss(loss_history, batch_history, args.save_path)
            if plot_path:
                logger.info(f"ğŸ’¾ æŸå¤±å›¾ä¿å­˜åˆ°: {plot_path}")
            else:
                logger.info("âš ï¸ æŸå¤±å›¾ç”Ÿæˆå¤±è´¥")
        
        # NEW: Save raw loss data
        if loss_history:
            logger.info(f"\nğŸ“Š ä¿å­˜åŸå§‹æŸå¤±æ•°æ®...")
            data_path = save_loss_data(loss_history, batch_history, args.save_path)
            if data_path:
                logger.info(f"ğŸ’¾ åŸå§‹æŸå¤±æ•°æ®ä¿å­˜åˆ°: {data_path}")
            else:
                logger.info("âš ï¸ åŸå§‹æŸå¤±æ•°æ®ä¿å­˜å¤±è´¥")
        
        logger.info(f"\nğŸ‰ é€‚é…å™¨è®­ç»ƒå®Œæˆï¼")
        logger.info(f"ğŸ“ æœ€ç»ˆæ¨¡å‹ä¿å­˜åˆ°: {args.save_path}")
        logger.info(f"ğŸ”¢ æ€»è®­ç»ƒå‚æ•°: {trainable_params:,}")
        logger.info(f"ğŸ“Š è®­ç»ƒç»Ÿè®¡: {args.epochs} epochs Ã— {args.batches_per_epoch} batches")
        
        # NEW: Add loss progression statistics
        if loss_history:
            initial_loss = loss_history[0]
            final_loss = loss_history[-1]
            min_loss = min(loss_history)
            max_loss = max(loss_history)
            loss_reduction = ((initial_loss - final_loss) / initial_loss) * 100
            
            logger.info(f"\nğŸ“ˆ æŸå¤±ç»Ÿè®¡:")
            logger.info(f"   åˆå§‹æŸå¤±: {initial_loss:.6f}")
            logger.info(f"   æœ€ç»ˆæŸå¤±: {final_loss:.6f}")
            logger.info(f"   æœ€ä½æŸå¤±: {min_loss:.6f}")
            logger.info(f"   æœ€é«˜æŸå¤±: {max_loss:.6f}")
            logger.info(f"   æŸå¤±é™ä½: {loss_reduction:.2f}%")
            logger.info(f"   è®°å½•æ‰¹æ¬¡: {len(loss_history)}")
        
        return True
        
    except Exception as e:
        logger.info(f"é€‚é…å™¨è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    finally:
        # NEW: Flush the logger to ensure all messages are written
        logger.flush()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

if __name__ == "__main__":
    mp.set_start_method('spawn', force=True)
    success = main()
    exit(0 if success else 1)